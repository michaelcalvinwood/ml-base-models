{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Decision Boundary\n",
        "\n",
        "https://psrivasin.medium.com/plotting-decision-boundaries-using-numpy-and-matplotlib-f5613d8acd19#:~:text=A%20decision%20boundary%20is%20a,side%20of%20the%20decision%20boundary.\n"
      ],
      "metadata": {
        "id": "SOZsT1UYqA1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Multiple Models"
      ],
      "metadata": {
        "id": "OjcoSvI0qNkm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xx1xeBk75n1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find ideal learning rate"
      ],
      "metadata": {
        "id": "ylM09H6GOagR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_ideal_learning_rate_binary_classification(model, X, y, max_lr=10):\n",
        "  model_lr = tf.keras.models.clone_model(model)\n",
        "  # Compile the model\n",
        "  model_lr.compile(loss=\"binary_crossentropy\", # we can use strings here too\n",
        "                  optimizer=\"Adam\", # same as tf.keras.optimizers.Adam() with default settings\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "  # Create a learning rate scheduler callback\n",
        "  lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n",
        "\n",
        "  # Fit the model (passing the lr_scheduler callback)\n",
        "  history = model_lr.fit(X,\n",
        "                          y,\n",
        "                          epochs=10*max_lr,\n",
        "                          callbacks=[lr_scheduler],\n",
        "                          verbose=0)\n",
        "  # Plot the learning rate versus the loss\n",
        "  lrs = 1e-4 * (10 ** (np.arange(100)/20))\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\n",
        "  plt.xlabel(\"Learning Rate\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Learning rate vs. loss\");\n",
        "\n",
        "#find_ideal_learning_rate_binary_classification(model, X, y)"
      ],
      "metadata": {
        "id": "YHecZDOSOc79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}